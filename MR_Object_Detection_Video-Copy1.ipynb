{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Object Detection Video\n",
    "\n",
    "### Documentation:\n",
    "https://imageai.readthedocs.io/en/latest/\n",
    "https://imageai.readthedocs.io/en/latest/video/index.html\n",
    "https://towardsdatascience.com/object-detection-with-10-lines-of-code-d6cb4d86f606\n",
    "https://github.com/OlafenwaMoses/ImageAI/blob/master/imageai/Detection/VIDEO.md\n",
    "\n",
    "MRobalinho - 05-11-2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------\n",
    "#### In order to Install ImageAI see this page:\n",
    "\n",
    "https://imageai.readthedocs.io/en/latest/\n",
    "\n",
    "<br/>Installing ImageAI\n",
    "<br/>ImageAI requires that you have Python 3.5.1 or higher installed as well as some other Python libraries and frameworks. Before you install ImageAI, you must install the following dependencies.\n",
    "<br/>\n",
    "<br/>Download and install:\n",
    "<br/>Python 3.5.1 or higher, Download Python here: https://www.python.org/downloads/\n",
    "<br/>pip3 , Download PyPi here: https://pypi.org/project/pip/\n",
    "<br/>\n",
    "<br/>Use Command line CMD.exe as admnistrator\n",
    "<br/>\n",
    "<br/>Tensorflow 1.4.0 or higher  > pip3 install --upgrade tensorflow\n",
    "<br/>Numpy 1.13.1 or higher     \t> pip3 install numpy\n",
    "<br/>SciPy .19.1 or higher\t\t> pip3 install scipy\n",
    "<br/>OpenCV\t\t\t\t\t\t> pip3 install opencv-python\n",
    "<br/>Pillow\t\t\t\t\t\t> pip3 install pillow\n",
    "<br/>Matplotlib\t\t\t\t\t> pip3 install matplotlib\n",
    "<br/>h5py\t\t\t\t\t\t> pip3 install h5py\n",
    "<br/>Keras\t\t\t\t\t\t> pip3 install keras\n",
    "<br/>Install ImageAI  \t\t\t> pip3 install https://github.com/OlafenwaMoses/ImageAI/releases/download/2.0.2/imageai-2.0.2-py3-none-any.whl\n",
    "\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "from imageai.Detection import VideoObjectDetection\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VideoObjectDetection class provides you function to detect objects in videos and live-feed from device cameras and IP cameras, using pre-trained models that was trained on the COCO dataset. The models supported are RetinaNet, YOLOv3 and TinyYOLOv3. This means you can detect and recognize 80 different kind of common everyday objects in any video. To get started, download any of the pre-trained model that you want to use via the links below.\n",
    "\n",
    "<br/>Download RetinaNet Model - resnet50_coco_best_v2.0.1.h5\n",
    "<br/>https://github.com/OlafenwaMoses/ImageAI/releases/tag/1.0/\n",
    "<br/>Download YOLOv3 Model - yolo.h5\n",
    "<br/>https://github.com/OlafenwaMoses/ImageAI/releases/tag/1.0/\n",
    "<br/>Download TinyYOLOv3 Model - yolo-tiny.h5\n",
    "<br/>https://github.com/OlafenwaMoses/ImageAI/releases/tag/1.0/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the video analysis, we need to do is specify a function, state the corresponding parameters it will be receiving and parse the function name into the per_frame_function, per_second_function, per_minute_function and video_complete_function parameters in the detection function. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any function we parse into the <b> per_frame_function</b>, the function will be executed after every single video frame is processed and he following will be parsed into it: \n",
    ">> Frame Index : This is the position number of the frame inside the video (e.g 1 for first frame and 20 for twentieth frame).\n",
    "\n",
    ">> Output Array : This is an array of dictionaries. Each dictionary corresponds to each detected object in the image and it contains the \"name\", \"percentage_probabaility\" and \"box_points\"(x1,y1,x2,y2) values of the object. \n",
    "\n",
    ">> Output Count : This is a dictionary that has the name of each unique object detected as its keys and the number of instances of the objects detected as the values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above result, the video was processed and saved in 10 frames per second (FPS). For any function you parse into the <b>per_second_function</b>, the function will be executed after every single second of the video that is processed and he following will be parsed into it: \n",
    ">> Second Index : This is the position number of the second inside the video (e.g 1 for first second and 20 for twentieth second).\n",
    "\n",
    ">> Output Array : This is an array of arrays, with each contained array and its position (array index + 1) corresponding to the equivalent frame in the last second of the video. Each contained array contains dictionaries. Each dictionary corresponds to each detected object in the image and it contains the \"name\", \"percentage_probabaility\" and \"box_points\"(x1,y1,x2,y2) values of the object. \n",
    "\n",
    ">> Count arrays : This is an array of dictionaries. Each dictionary and its position (array index + 1) corresponds to the equivalent frame in the last second of he video. Each dictionary has the name of each unique object detected as its keys and the number of instances of the objects detected as the values. \n",
    "\n",
    ">> Average Output Count : This is a dictionary that has the name of each unique object detected in the last second as its keys and the average number of instances of the objects detected across the number of frames as the values. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forFrame(frame_number, output_array, output_count, detected_copy):\n",
    "    print(\"FOR FRAME \" , frame_number)\n",
    "#    print(\"Output for each object : \", output_array)\n",
    "    print(\"Output count for unique objects : \", output_count)\n",
    "    print(\"------------END OF A FRAME --------------\")\n",
    "\n",
    "def forSeconds(second_number, output_arrays, count_arrays, average_output_count, detected_copy):\n",
    "    print(\"SECOND : \", second_number)\n",
    "#    print(\"Array for the outputs of each frame \", output_arrays)\n",
    "    print(\"Array for output count for unique objects in each frame : \", count_arrays)\n",
    "    print(\"Output average count for unique objects in the last second: \", average_output_count)\n",
    "    print(\"------------END OF A SECOND --------------\")\n",
    "\n",
    "def forMinute(minute_number, output_arrays, count_arrays, average_output_count, detected_copy):\n",
    "    print(\"MINUTE : \", minute_number)\n",
    "#    print(\"Array for the outputs of each frame \", output_arrays)\n",
    "    print(\"Array for output count for unique objects in each frame : \", count_arrays)\n",
    "    print(\"Output average count for unique objects in the last minute: \", average_output_count)\n",
    "    print(\"------------END OF A MINUTE --------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Lenovo\\\\ML\\\\Object-Detection-Video'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to execute\n",
    "yolo_path = 'TinyYOLOv3_Model/'\n",
    "video_path = 'Video/'\n",
    "execution_path = os.getcwd()\n",
    "execution_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Video/'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Video to Use\n",
    "use_video = \"traffic-mini.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Frame :  1\n",
      "FOR FRAME  1\n",
      "Output count for unique objects :  {'motorcycle': 1, 'car': 4, 'person': 7}\n",
      "------------END OF A FRAME --------------\n",
      "Processing Frame :  2\n",
      "FOR FRAME  2\n",
      "Output count for unique objects :  {'motorcycle': 1, 'car': 3, 'person': 6}\n",
      "------------END OF A FRAME --------------\n",
      "Processing Frame :  3\n",
      "FOR FRAME  3\n",
      "Output count for unique objects :  {'motorcycle': 1, 'car': 5, 'person': 6}\n",
      "------------END OF A FRAME --------------\n",
      "Processing Frame :  4\n",
      "FOR FRAME  4\n",
      "Output count for unique objects :  {'motorcycle': 1, 'car': 2, 'person': 6}\n",
      "------------END OF A FRAME --------------\n",
      "Processing Frame :  5\n"
     ]
    }
   ],
   "source": [
    "# Model to Use\n",
    "use_model = \"yolo-tiny.h5\"\n",
    "\n",
    "analisys_video = \"detected_\" + use_video\n",
    "\n",
    "detector = VideoObjectDetection()\n",
    "\n",
    "# Iniciate model\n",
    "#detector.setModelTypeAsYOLOv3()      # To YOLOv3 model\n",
    "#detector.setModelTypeAsRetinaNet()   # To Retinanet model\n",
    "detector.setModelTypeAsTinyYOLOv3()   # To TinyYOLOv3 model\n",
    "\n",
    "detector.setModelPath( os.path.join(yolo_path , use_model))\n",
    "detector.loadModel()\n",
    "\n",
    "vid_path = detector.detectObjectsFromVideo(input_file_path=os.path.join( video_path, use_video),\n",
    "                                           output_file_path=os.path.join(video_path, analisys_video),\n",
    "                                           frames_per_second=20,\n",
    "                                 #          per_second_function = forSeconds,\n",
    "                                           per_frame_function = forFrame,\n",
    "                                 #          per_minute_function= forMinute,\n",
    "                                           minimum_percentage_probability=30,\n",
    "                                           log_progress=True,\n",
    "                                           return_detected_frame=True)\n",
    "                                           \n",
    " \n",
    "print(vid_path)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Finished...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
